{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOeroalFdoCj"
   },
   "source": [
    "# **Bantu Language Modeling**\n",
    "\n",
    "The project is divided in 4 parts\n",
    "\n",
    "\n",
    "*   Scratch RNN for Swahili corpus \n",
    "*   Scratch RNN for Kwere corpus\n",
    "*   Anything Goes for Swahili corpus \n",
    "*   Anything Goes for Kwere corpus\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "**Note :**\n",
    "> The ipynb file I created is ported from .py file. Due to training related isuues at SLU Jupyter Hub and Google Colab, I used Dr. Hou server to train the model. I used *nohup* to keep training running on the server. \n",
    "\n",
    "\n",
    "> **Loss (Cross Entropy):** I exported graph while training the model which is shown below after the coding blocks for CWE and SW. \n",
    "\n",
    "Due to big training size I haven't got chance to play with the hyperparameters (batch size and number of itertation) for the model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1hrvOJBfAuC"
   },
   "source": [
    "# **Scratch RNN for Swahili corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9prVoxCMdnFW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / np.sum(e_x)\n",
    "\n",
    "\n",
    "def cross_entropy(x, index):\n",
    "    \"\"\"\n",
    "    Assumption: the ground truth vector contains only one non-zero component with a value of 1\n",
    "    \"\"\"\n",
    "\n",
    "    loss = - np.log2(x[index]) if x[index] > 0 else 0\n",
    "    return loss\n",
    "\n",
    "\n",
    "def cross_entropy_d(x, index):\n",
    "    \"\"\"\n",
    "    Assumption: the ground truth vector contains only one non-zero component with a value of 1\n",
    "    \"\"\"\n",
    "\n",
    "    x[index] -= 1\n",
    "    return x\n",
    "\n",
    "\n",
    "def char_to_ix(chars):\n",
    "    \"\"\"\n",
    "    Make a dictionary that maps a character to an index\n",
    "    Arguments:\n",
    "        chars -- list of character set\n",
    "    Returns:\n",
    "        dictionary that maps a character to an index\n",
    "    \"\"\"\n",
    "\n",
    "    return {ch: i for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "def ix_to_char(chars):\n",
    "    \"\"\"\n",
    "    Make a dictionary that maps an index to a character\n",
    "    Arguments:\n",
    "        chars -- list of character set\n",
    "    Returns:\n",
    "        dictionary that maps an index to a character\n",
    "    \"\"\"\n",
    "\n",
    "    return {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "def one_hot(data, ch2ix):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data -- string\n",
    "        ch2ix -- dictionary that maps a character to an index\n",
    "    Returns:\n",
    "        Numpy array, shape = (len(data), len(ch2ix), 1)\n",
    "    \"\"\"\n",
    "\n",
    "    result = []\n",
    "    for i in range(len(data)):\n",
    "        x = np.zeros((len(ch2ix), 1))\n",
    "        if data[i] is not None:\n",
    "            x[ch2ix[data[i]], 0] = 1\n",
    "            result.append(x)\n",
    "\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "def initialize_xavier(first, second):\n",
    "    \"\"\"\n",
    "    Xavier initialization\n",
    "    Arguments:\n",
    "        first -- first dimension size\n",
    "        second -- second dimension size\n",
    "    Returns:\n",
    "        W -- Weight matrix initialized by Xavier method\n",
    "    \"\"\"\n",
    "\n",
    "    sd = np.sqrt(2.0 / (first + second))\n",
    "    W = np.random.randn(first, second) * sd\n",
    "\n",
    "    return W\n",
    "\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self, xlabel, ylabel):\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.ion()\n",
    "        plt.show()\n",
    "\n",
    "    def update(self, x, y, img_name='Figure'):\n",
    "        plt.plot(x, y, color='xkcd:royal blue')\n",
    "        plt.show()\n",
    "        plt.savefig('./figure/' + img_name + '.png')\n",
    "        plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIb4b14CdnFc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, input_size, output_size, hidden_size, cell_length, depth_size=1, batch_size=1, drop_rate=0):\n",
    "        self._input_size = input_size\n",
    "        self._output_size = output_size\n",
    "        self._hidden_size = hidden_size\n",
    "        self._cell_length = cell_length\n",
    "        self._depth_size = depth_size\n",
    "        self._batch_size = batch_size\n",
    "        self._drop_rate = drop_rate\n",
    "\n",
    "        # Xavier initialization\n",
    "        self._parameters = {'W_xa': [initialize_xavier(self._input_size if d == 0 else self._hidden_size, self._hidden_size) for d in range(self._depth_size)],\n",
    "                            'W_aa': [initialize_xavier(self._hidden_size, self._hidden_size) for d in range(self._depth_size)],\n",
    "                            'W_ay': [initialize_xavier(self._hidden_size, self._output_size)],\n",
    "                            'b_a': [np.zeros((1, self._hidden_size)) for d in range(self._depth_size)],\n",
    "                            'b_y': [np.zeros((1, self._output_size))],\n",
    "                            'a': [np.zeros((self._batch_size, self._hidden_size)) for d in range(self._depth_size)]}\n",
    "\n",
    "        self._gradients = {'dW_xa': [np.zeros_like(self._parameters['W_xa'][d]) for d in range(self._depth_size)],\n",
    "                           'dW_aa': [np.zeros_like(self._parameters['W_aa'][d]) for d in range(self._depth_size)],\n",
    "                           'dW_ay': [np.zeros_like(self._parameters['W_ay'][0])],\n",
    "                           'db_a': [np.zeros_like(self._parameters['b_a'][d]) for d in range(self._depth_size)],\n",
    "                           'db_y': [np.zeros_like(self._parameters['b_y'][0])],\n",
    "                           'da': [np.zeros_like(self._parameters['a'][d]) for d in range(self._depth_size)]}\n",
    "\n",
    "        self._momentums = {'dW_xa': [np.ones_like(self._gradients['dW_xa'][d]) * 0.1 for d in range(self._depth_size)],\n",
    "                           'dW_aa': [np.zeros_like(self._gradients['dW_aa'][d]) * 0.1 for d in range(self._depth_size)],\n",
    "                           'dW_ay': [np.zeros_like(self._gradients['dW_ay'][0]) * 0.1],\n",
    "                           'db_a': [np.zeros_like(self._gradients['db_a'][d]) * 0.1 for d in range(self._depth_size)],\n",
    "                           'db_y': [np.zeros_like(self._gradients['db_y'][0]) * 0.1]}\n",
    "\n",
    "        self._loss = - np.log(1.0 / self._output_size) * self._cell_length\n",
    "\n",
    "    def optimize(self, X, Y, learning_rate=0.01):\n",
    "        cache = self.forward(X)\n",
    "        self.backward(Y, cache)\n",
    "        self.update_parameters(learning_rate=learning_rate)\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Computes only the forward pass through one step of the time\n",
    "        -input is an index of the char in vocabulary\n",
    "\n",
    "        Returns probabilities and the updated state of the hidden units\n",
    "        '''\n",
    "        self._loss = 0\n",
    "\n",
    "        x, y_hat = [{} for d in range(self._depth_size + 1)], {}\n",
    "        a = [{-1: np.copy(self._parameters['a'][d])} for d in range(self._depth_size)]\n",
    "        dropout = [{} for d in range(self._depth_size)]\n",
    "\n",
    "        for t in range(self._cell_length):\n",
    "            x[0][t] = X[:, t, :, :].reshape(self._batch_size, self._input_size)\n",
    "\n",
    "            for d in range(self._depth_size):\n",
    "                dropout[d][t] = np.random.binomial(1, 1 - self._drop_rate, (1, self._hidden_size)) / (1 - self._drop_rate)\n",
    "                a[d][t] = np.tanh(np.dot(x[d][t], self._parameters['W_xa'][d]) +\n",
    "                                  np.dot(a[d][t - 1], self._parameters['W_aa'][d]) +\n",
    "                                  self._parameters['b_a'][d])\n",
    "                x[d + 1][t] = np.copy(a[d][t]) * dropout[d][t]\n",
    "\n",
    "            z = np.dot(x[self._depth_size][t], self._parameters['W_ay'][0]) + self._parameters['b_y'][0]\n",
    "            z = np.clip(z, -100, 100)\n",
    "            y_hat[t] = np.array([softmax(z[b, :]) for b in range(self._batch_size)])\n",
    "\n",
    "        cache = (x, a, y_hat, dropout)\n",
    "        return cache\n",
    "\n",
    "    def backward(self, Y, cache):\n",
    "        self._gradients = {key: [np.zeros_like(self._gradients[key][d]) for d in range(len(self._gradients[key]))] for key in self._gradients.keys()}\n",
    "        (x, a, y_hat, dropout) = cache\n",
    "\n",
    "        for t in reversed(range(self._cell_length)):\n",
    "            self._loss += sum([cross_entropy(y_hat[t][b, :], Y[b, t]) for b in range(self._batch_size)]) / (self._cell_length * self._batch_size)\n",
    "            dy = np.array([cross_entropy_d(y_hat[t][b, :], Y[b, t]) for b in range(self._batch_size)]) / (self._cell_length * self._batch_size)\n",
    "\n",
    "            self._gradients['dW_ay'][0] += np.dot(x[self._depth_size][t].T, dy)\n",
    "            self._gradients['db_y'][0] += dy.sum(axis=0)\n",
    "            da = np.dot(dy, self._parameters['W_ay'][0].T)\n",
    "\n",
    "            for d in reversed(range(self._depth_size)):\n",
    "                da = (1 - a[d][t] ** 2) * (da * dropout[d][t] + self._gradients['da'][d])\n",
    "                self._gradients['dW_xa'][d] += np.dot(x[d][t].T, da)\n",
    "                self._gradients['dW_aa'][d] += np.dot(a[d][t - 1].T, da)\n",
    "                self._gradients['db_a'][d] += da.sum(axis=0)\n",
    "                self._gradients['da'][d] = np.dot(da, self._parameters['W_aa'][d].T)\n",
    "                da = np.dot(da, self._parameters['W_xa'][d].T)\n",
    "\n",
    "        self._parameters['a'] = [a[d][self._cell_length - 1] for d in range(self._depth_size)]\n",
    "\n",
    "    def update_parameters(self, learning_rate=0.01):\n",
    "        parameters = self._parameters['W_xa'] + self._parameters['W_aa'] + self._parameters['W_ay'] + self._parameters['b_a'] + self._parameters['b_y']\n",
    "        gradients = self._gradients['dW_xa'] + self._gradients['dW_aa'] + self._gradients['dW_ay'] + self._gradients['db_a'] + self._gradients['db_y']\n",
    "        momentums = self._momentums['dW_xa'] + self._momentums['dW_aa'] + self._momentums['dW_ay'] + self._momentums['db_a'] + self._momentums['db_y']\n",
    "\n",
    "        for w, g, m in zip(parameters, gradients, momentums):\n",
    "            np.clip(w, -1, 1, out=w)\n",
    "\n",
    "            # # Adagrad\n",
    "            # m += g ** 2\n",
    "            # w -= learning_rate * g / np.sqrt(m + 1e-8)\n",
    "\n",
    "            # RMSProp\n",
    "            m = 0.9 * m + 0.1 * g ** 2\n",
    "            w -= learning_rate * g / np.sqrt(m + 1e-8)\n",
    "\n",
    "    def sample(self, ix, n):\n",
    "        '''\n",
    "        Samples the model, returns the sample of length N as a string\n",
    "        '''\n",
    "        ixes = [ix]\n",
    "        a = [np.zeros((1, self._hidden_size)) for d in range(self._depth_size)]\n",
    "        for t in range(n):\n",
    "            x = np.zeros((1, self._input_size))\n",
    "            x[0, ix] = 1\n",
    "\n",
    "            for d in range(self._depth_size):\n",
    "                a[d] = np.tanh(np.dot(x, self._parameters['W_xa'][d]) +\n",
    "                               np.dot(a[d], self._parameters['W_aa'][d]) +\n",
    "                               self._parameters['b_a'][d])\n",
    "                x = a[d]\n",
    "\n",
    "            z = np.dot(x, self._parameters['W_ay']) + self._parameters['b_y']\n",
    "            z = np.clip(z, -100, 100)\n",
    "            y = softmax(z / 0.7)\n",
    "\n",
    "            ix = np.random.choice(range(self._input_size), p=y.ravel())\n",
    "            ixes.append(ix)\n",
    "\n",
    "        return ixes\n",
    "\n",
    "    def initialize_optimizer(self):\n",
    "        self._momentums = {key: [np.ones_like(self._momentums[key][d]) * 0.1 for d in range(len(self._momentums[key]))] for key in self._momentums.keys()}\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        self._parameters['a'] = [np.zeros_like(self._parameters['a'][d]) for d in range(self._depth_size)]\n",
    "\n",
    "    def hidden_state(self):\n",
    "        return self._parameters['a']\n",
    "\n",
    "    def loss(self):\n",
    "        return self._loss\n",
    "\n",
    "    def parameters(self):\n",
    "        return self._parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vgflCnPdnFg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from utils import char_to_ix, ix_to_char, one_hot, Graph\n",
    "# from model import RNN\n",
    "import pickle\n",
    "import timeit\n",
    "\n",
    "\n",
    "def model(data='input.txt', hidden_size=256, seq_length=200, depth_size=1, batch_size=64, drop_rate=0.1,\n",
    "          num_iteration=100, learning_rate=0.01, img_name='Figure'):\n",
    "    # Open a training text file\n",
    "    data = open(data, 'rb').read().decode('UTF-8')\n",
    "    chars = list(set(data))\n",
    "    chars.sort()\n",
    "    data_size, vocab_size = len(data), len(chars)\n",
    "    print('Data has %d total characters, %d unique characters.' % (data_size, vocab_size))\n",
    "\n",
    "    # Make a dictionary that maps {character:index} and {index:character}\n",
    "    ch2ix, ix2ch = char_to_ix(chars), ix_to_char(chars)\n",
    "\n",
    "    # Set RNN model\n",
    "    model = RNN(vocab_size, vocab_size, hidden_size, seq_length, depth_size, batch_size, drop_rate)\n",
    "\n",
    "    cnt = 0\n",
    "    losses = {}\n",
    "    graph = Graph('Iteration', 'Loss')\n",
    "\n",
    "    # Optimize model\n",
    "    start = timeit.default_timer()\n",
    "    for n in range(num_iteration):\n",
    "        model.initialize_hidden_state()\n",
    "        model.initialize_optimizer()\n",
    "\n",
    "        # Split text by mini-batch with batch_size\n",
    "        batch_length = data_size // batch_size\n",
    "        for i in range(0, batch_length - seq_length, seq_length):\n",
    "            mini_batch_X, mini_batch_Y = [], []\n",
    "\n",
    "            for j in range(0, data_size - batch_length + 1, batch_length):\n",
    "                mini_batch_X.append(one_hot(data[j + i:j + i + seq_length], ch2ix))\n",
    "                mini_batch_Y.append([ch2ix[ch] for ch in data[j + i + 1:j + i + seq_length + 1]])\n",
    "\n",
    "            mini_batch_X = np.array(mini_batch_X)\n",
    "            mini_batch_Y = np.array(mini_batch_Y)\n",
    "\n",
    "            model.optimize(mini_batch_X, mini_batch_Y, learning_rate=learning_rate)\n",
    "\n",
    "            cnt += 1\n",
    "            if cnt % 100 == 0 or cnt == 1:\n",
    "                stop = timeit.default_timer()\n",
    "\n",
    "                loss = model.loss()\n",
    "                losses[cnt] = loss\n",
    "\n",
    "                print(\"\\n######################################\")\n",
    "                print(\"Total iteration: %d\" % (n + 1))\n",
    "                print(\"Iteration: %d\" % cnt)\n",
    "                print(\"Loss: %f\" % loss)\n",
    "                print(\"Time: %f\" % (stop - start))\n",
    "\n",
    "                ix = np.random.randint(0, vocab_size)\n",
    "                sample_ixes = model.sample(ix, 200)\n",
    "                txt = ''.join(ix2ch[ix] for ix in sample_ixes)\n",
    "                print(\"\\n### Starts Here ###\\n\\n\" + txt.rstrip() + \"\\n\\n### Ends Here ###\")\n",
    "                print(\"######################################\")\n",
    "\n",
    "                graph_x = np.array(sorted(losses))\n",
    "                graph_y = np.array([losses[key] for key in graph_x])\n",
    "                graph.update(graph_x, graph_y, img_name=img_name)\n",
    "\n",
    "    return model, ch2ix, ix2ch\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ##########\n",
    "    data = 'sw-train'\n",
    "    num_iteration = 100\n",
    "    optimizer = 'adagrad'\n",
    "    ##########\n",
    "\n",
    "    infile = data + '.txt'\n",
    "    outfile = data + '_' + str(num_iteration) + '_' + optimizer\n",
    "\n",
    "    result, ch2ix, ix2ch = model(data=infile, num_iteration=num_iteration, img_name=outfile)\n",
    "\n",
    "    file = open('./result/' + outfile + '.pickle', 'wb')\n",
    "    pickle.dump(result, file)\n",
    "    pickle.dump(ch2ix, file)\n",
    "    pickle.dump(ix2ch, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nt9Ca_8WfJy0"
   },
   "source": [
    "# **Scratch RNN for Kwere corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFG6jrRudnFl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import timeit\n",
    "\n",
    "final_losss = []\n",
    "totalcount = 0\n",
    "\n",
    "def model(data='cwe-train.txt', hidden_size=256, seq_length=200, depth_size=2, batch_size=10, drop_rate=0.1,\n",
    "          num_iteration=100, learning_rate=0.01, img_name='Figure'):\n",
    "    # Open a training text file\n",
    "    data = open(data, 'rb').read().decode('UTF-8')\n",
    "    chars = list(set(data))\n",
    "    chars.sort()\n",
    "    data_size, vocab_size = len(data), len(chars)\n",
    "    print('Data has %d total characters, %d unique characters.' % (data_size, vocab_size))\n",
    "\n",
    "    # Make a dictionary that maps {character:index} and {index:character}\n",
    "    ch2ix, ix2ch = char_to_ix(chars), ix_to_char(chars)\n",
    "\n",
    "    # Set RNN model\n",
    "    model = RNN(vocab_size, vocab_size, hidden_size, seq_length, depth_size, batch_size, drop_rate)\n",
    "\n",
    "    cnt = 0\n",
    "    losses = {}\n",
    "    graph = Graph('Iteration', 'Loss')\n",
    "\n",
    "    # Optimize model\n",
    "    start = timeit.default_timer()\n",
    "    for n in range(num_iteration):\n",
    "        model.initialize_hidden_state()\n",
    "        model.initialize_optimizer()\n",
    "\n",
    "        # Split text by mini-batch with batch_size\n",
    "        batch_length = data_size // batch_size\n",
    "        for i in range(0, batch_length - seq_length, seq_length):\n",
    "            mini_batch_X, mini_batch_Y = [], []\n",
    "\n",
    "            for j in range(0, data_size - batch_length + 1, batch_length):\n",
    "                mini_batch_X.append(one_hot(data[j + i:j + i + seq_length], ch2ix))\n",
    "                mini_batch_Y.append([ch2ix[ch] for ch in data[j + i + 1:j + i + seq_length + 1]])\n",
    "\n",
    "            mini_batch_X = np.array(mini_batch_X)\n",
    "            mini_batch_Y = np.array(mini_batch_Y)\n",
    "\n",
    "            model.optimize(mini_batch_X, mini_batch_Y, learning_rate=learning_rate)\n",
    "\n",
    "            cnt += 1\n",
    "            totalcount = cnt\n",
    "            if cnt % 100 == 0 or cnt == 1:\n",
    "                stop = timeit.default_timer()\n",
    "\n",
    "                loss = model.loss()\n",
    "                losses[cnt] = loss\n",
    "                final_losss = losses\n",
    "\n",
    "                print(\"\\n######################################\")\n",
    "                print(\"Total iteration: %d\" % (n + 1))\n",
    "                print(\"Iteration: %d\" % cnt)\n",
    "                print(\"Loss: %f\" % loss)\n",
    "                print(\"Time: %f\" % (stop - start))\n",
    "\n",
    "                ix = np.random.randint(0, vocab_size)\n",
    "                sample_ixes = model.sample(ix, 200)\n",
    "                txt = ''.join(ix2ch[ix] for ix in sample_ixes)\n",
    "                print(\"\\n### Starts Here ###\\n\\n\" + txt.rstrip() + \"\\n\\n### Ends Here ###\")\n",
    "                print(\"######################################\")\n",
    "\n",
    "                graph_x = np.array(sorted(losses))\n",
    "                graph_y = np.array([losses[key] for key in graph_x])\n",
    "                graph.update(graph_x, graph_y, img_name=img_name)\n",
    "\n",
    "    return model, ch2ix, ix2ch\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ##########\n",
    "    data = 'cwe-train'\n",
    "    num_iteration = 100\n",
    "    optimizer = 'adagrad'\n",
    "    ##########\n",
    "\n",
    "    infile = data + '.txt'\n",
    "    outfile = data + '_' + str(num_iteration) + '_' + optimizer\n",
    "\n",
    "    result, ch2ix, ix2ch = model(data=infile, num_iteration=num_iteration, img_name=outfile)\n",
    "\n",
    "    file = open('./result/' + outfile + '.pickle', 'wb')\n",
    "    pickle.dump(result, file)\n",
    "    pickle.dump(ch2ix, file)\n",
    "    pickle.dump(ix2ch, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tTPZQhUfQ3F"
   },
   "source": [
    "Cross Entropy for SW corpus : 1.527769\n",
    "\n",
    "![](https://i.ibb.co/W3wk7Mv/Screenshot-from-2020-10-11-11-30-47.png)\n",
    "\n",
    "Cross Entropy for CWE corpus : 2.165388\n",
    "\n",
    "![](https://i.ibb.co/BZqKstj/Screenshot-from-2020-10-11-11-29-19.png)\n",
    "\n",
    "\n",
    "\n",
    "**Training Logs are uploaded in the repository in result folder**\n",
    "\n",
    "**Cross Entropy : SW Corpus**\n",
    "\n",
    "\n",
    "![](https://i.ibb.co/vhd5J8P/sw-train-100-adagrad-Log2-1.png)\n",
    "\n",
    "**Cross Entropy : CWE Corpus**\n",
    "\n",
    "![](https://i.ibb.co/cgfX8CC/cwe-train-100-adagrad-Log2-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugv63Az6i9ww"
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BantuLM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
