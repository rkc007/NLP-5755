{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s1Q0dvWOvDev"
   },
   "source": [
    "# **Sentiment Analysis**\n",
    "\n",
    "![sentiment.png](https://cdn.brandmentions.com/blog/wp-content/uploads/2019/05/sentiment-analysys-brandmentions.png)\n",
    "\n",
    "> The code here are divided in 2 parts\n",
    "\n",
    "\n",
    "*   Done from scratch\n",
    "*   Used different algorithms to compare results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ydAYFAe9wE1C"
   },
   "source": [
    "All the important libraries are loaded in first step\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vYzcZ70WvBvv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import re\n",
    "import json, nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB  # Naive Bayes Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h-LwzPJmwbTG"
   },
   "source": [
    "# **Naive Bayes from Scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "id": "oUjpbAU7wwN1",
    "outputId": "a15d1f82-e8d5-4d73-bf89-bf881db1d8ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 55429: Expected 2 fields in line 55429, saw 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@USER @USER a sicrhau bod mwy o arian poced 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Parti Dolig da gyda tim swyddfa canolog @USER ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@USER yeaah ma fe yn wir. ( oh well.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>@USER hahaha idk. 3am oedd y bws ti?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@USER dwim yn gal llun ohoni?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                             review\n",
       "0         0  @USER @USER a sicrhau bod mwy o arian poced 'd...\n",
       "1         1  Parti Dolig da gyda tim swyddfa canolog @USER ...\n",
       "2         0               @USER yeaah ma fe yn wir. ( oh well.\n",
       "3         1               @USER hahaha idk. 3am oedd y bws ti?\n",
       "4         0                      @USER dwim yn gal llun ohoni?"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    '''\n",
    "        Data Loading:\n",
    "        ----------\n",
    "        The data is loaded from a tsv file\n",
    "        ---------\n",
    "\n",
    "        Line Skipping : \n",
    "        ----------\n",
    "        There were few bugs in the dataset like pandas was not able to load quotes in a the same line so \"quoting=csv.QUOTE_NONE\" was used.\n",
    "    '''\n",
    "    \n",
    "training_set = pd.read_csv('./train.tsv', error_bad_lines=False,quoting=csv.QUOTE_NONE, sep='\\t', header=None, engine='python')\n",
    "training_set\n",
    "training_set.columns = ['sentiment', 'review']\n",
    "training_set = training_set[pd.notnull(training_set['review'])]\n",
    "training_set.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UYvLGzcivBv1"
   },
   "outputs": [],
   "source": [
    "def preprocess_string(str_arg):\n",
    "    \n",
    "    \"\"\"\"\n",
    "        Parameters:\n",
    "        ----------\n",
    "        str_arg: example string to be preprocessed\n",
    "        \n",
    "        What the function does?\n",
    "        -----------------------\n",
    "        Preprocess the string argument - str_arg - such that :\n",
    "        1. everything apart from letters is excluded\n",
    "        2. multiple spaces are replaced by single space\n",
    "        3. str_arg is converted to lower case \n",
    "        \n",
    "        Example:\n",
    "        --------\n",
    "        Input :  Menu is absolutely perfect,loved it!\n",
    "        Output:  ['menu', 'is', 'absolutely', 'perfect', 'loved', 'it']\n",
    "        \n",
    "\n",
    "        Returns:\n",
    "        ---------\n",
    "        Preprocessed string \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    cleaned_str=re.sub('[^a-z\\s]+',' ',str_arg,flags=re.IGNORECASE) #every char except alphabets is replaced\n",
    "    cleaned_str=re.sub('(\\s+)',' ',cleaned_str) #multiple spaces are replaced by single space\n",
    "    cleaned_str=cleaned_str.lower() #converting the cleaned string to lower case\n",
    "    \n",
    "    return cleaned_str # eturning the preprocessed string in tokenized form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AmSDKXiPvBv5"
   },
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self,unique_classes):\n",
    "        \n",
    "        self.classes=unique_classes # Constructor is simply passed with unique number of classes of the training set\n",
    "        \n",
    "\n",
    "    def addToBow(self,example,dict_index):\n",
    "        \n",
    "        '''\n",
    "            Parameters:\n",
    "            1. example \n",
    "            2. dict_index - implies to which BoW category this example belongs to\n",
    "\n",
    "            What the function does?\n",
    "            -----------------------\n",
    "            It simply splits the example on the basis of space as a tokenizer and adds every tokenized word to\n",
    "            its corresponding dictionary/BoW\n",
    "\n",
    "            Returns:\n",
    "            ---------\n",
    "            Nothing\n",
    "        \n",
    "       '''\n",
    "        \n",
    "        if isinstance(example,np.ndarray): example=example[0]\n",
    "     \n",
    "        for token_word in example.split(): #for every word in preprocessed example\n",
    "          \n",
    "            self.bow_dicts[dict_index][token_word]+=1 #increment in its count\n",
    "            \n",
    "    def train(self,dataset,labels):\n",
    "        \n",
    "        '''\n",
    "            Parameters:\n",
    "            1. dataset - shape = (m X d)\n",
    "            2. labels - shape = (m,)\n",
    "\n",
    "            What the function does?\n",
    "            -----------------------\n",
    "            This is the training function which will train the Naive Bayes Model i.e compute a BoW for each\n",
    "            category/class. \n",
    "\n",
    "            Returns:\n",
    "            ---------\n",
    "            Nothing\n",
    "        \n",
    "        '''\n",
    "    \n",
    "        self.examples=dataset\n",
    "        self.labels=labels\n",
    "        self.bow_dicts=np.array([defaultdict(lambda:0) for index in range(self.classes.shape[0])])\n",
    "        \n",
    "        #only convert to numpy arrays if initially not passed as numpy arrays - else its a useless recomputation\n",
    "        \n",
    "        if not isinstance(self.examples,np.ndarray): self.examples=np.array(self.examples)\n",
    "        if not isinstance(self.labels,np.ndarray): self.labels=np.array(self.labels)\n",
    "            \n",
    "        #constructing BoW for each category\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "          \n",
    "            all_cat_examples=self.examples[self.labels==cat] #filter all examples of category == cat\n",
    "            \n",
    "            #get examples preprocessed\n",
    "            \n",
    "            cleaned_examples=[preprocess_string(cat_example) for cat_example in all_cat_examples]\n",
    "            \n",
    "            cleaned_examples=pd.DataFrame(data=cleaned_examples)\n",
    "            \n",
    "            #now costruct BoW of this particular category\n",
    "            np.apply_along_axis(self.addToBow,1,cleaned_examples,cat_index)\n",
    "            \n",
    "                \n",
    "        ###################################################################################################\n",
    "        \n",
    "        '''\n",
    "            ------------------------------------------------------------------------------------\n",
    "            Computing : {for each word w [ count(w|c)+1 ] / [ count(c) + |V| + 1 ] } * p(c)\n",
    "            ------------------------------------------------------------------------------------\n",
    "            \n",
    "            We are done with constructing of BoW for each category. But we need to precompute a few \n",
    "            other calculations at training time too:\n",
    "            1. prior probability of each class - p(c)\n",
    "            2. vocabulary |V| \n",
    "            3. denominator value of each class - [ count(c) + |V| + 1 ] \n",
    "            \n",
    "        '''\n",
    "        \n",
    "        ###################################################################################################\n",
    "      \n",
    "        prob_classes=np.empty(self.classes.shape[0])\n",
    "        all_words=[]\n",
    "        cat_word_counts=np.empty(self.classes.shape[0])\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "           \n",
    "            #Calculating prior probability p(c) for each class\n",
    "            prob_classes[cat_index]=np.sum(self.labels==cat)/float(self.labels.shape[0]) \n",
    "            \n",
    "            #Calculating total counts of all the words of each class \n",
    "            count=list(self.bow_dicts[cat_index].values())\n",
    "            cat_word_counts[cat_index]=np.sum(np.array(list(self.bow_dicts[cat_index].values())))+1 # |v| is remaining to be added\n",
    "            \n",
    "            #get all words of this category                                \n",
    "            all_words+=self.bow_dicts[cat_index].keys()\n",
    "                                                     \n",
    "        \n",
    "        #combine all words of every category & make them unique to get vocabulary -V- of entire training set\n",
    "        \n",
    "        self.vocab=np.unique(np.array(all_words))\n",
    "        self.vocab_length=self.vocab.shape[0]\n",
    "                                  \n",
    "        #computing denominator value                                      \n",
    "        denoms=np.array([cat_word_counts[cat_index]+self.vocab_length+1 for cat_index,cat in enumerate(self.classes)])                                                                          \n",
    "      \n",
    "        '''\n",
    "            Every element of self.cats_info has a tuple of values\n",
    "            Each tuple has a dict at index 0, prior probability at index 1, denominator value at index 2\n",
    "        '''\n",
    "        \n",
    "        self.cats_info=[(self.bow_dicts[cat_index],prob_classes[cat_index],denoms[cat_index]) for cat_index,cat in enumerate(self.classes)]                               \n",
    "        self.cats_info=np.array(self.cats_info)                                 \n",
    "                                              \n",
    "                                              \n",
    "    def getExampleProb(self,test_example):                                \n",
    "        \n",
    "        '''\n",
    "            Parameters:\n",
    "            -----------\n",
    "            1. a single test example \n",
    "\n",
    "            What the function does?\n",
    "            -----------------------\n",
    "            Function that estimates posterior probability of the given test example\n",
    "\n",
    "            Returns:\n",
    "            ---------\n",
    "            probability of test example in ALL CLASSES\n",
    "        '''                                      \n",
    "                                              \n",
    "        likelihood_prob=np.zeros(self.classes.shape[0]) #to store probability w.r.t each class\n",
    "        \n",
    "        #finding probability w.r.t each class of the given test example\n",
    "        for cat_index,cat in enumerate(self.classes): \n",
    "                             \n",
    "            for test_token in test_example.split(): #split the test example and get p of each test word\n",
    "                \n",
    "                ####################################################################################\n",
    "                                              \n",
    "                #This loop computes : for each word w [ count(w|c)+1 ] / [ count(c) + |V| + 1 ]                               \n",
    "                                              \n",
    "                ####################################################################################                              \n",
    "                \n",
    "                #get total count of this test token from it's respective training dict to get numerator value                           \n",
    "                test_token_counts=self.cats_info[cat_index][0].get(test_token,0)+1\n",
    "                \n",
    "                #now get likelihood of this test_token word                              \n",
    "                test_token_prob=test_token_counts/float(self.cats_info[cat_index][2])                              \n",
    "                \n",
    "                #taking log: To prevent underflow!\n",
    "                likelihood_prob[cat_index]+=np.log(test_token_prob)\n",
    "                                              \n",
    "        # we have likelihood estimate of the given example against every class but we need posterior probility\n",
    "        post_prob=np.empty(self.classes.shape[0])\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "            post_prob[cat_index]=likelihood_prob[cat_index]+np.log(self.cats_info[cat_index][1])                                  \n",
    "      \n",
    "        return post_prob\n",
    "    \n",
    "   \n",
    "    def test(self,test_set):\n",
    "      \n",
    "        '''\n",
    "            Parameters:\n",
    "            -----------\n",
    "            1. A complete test set of shape (m,)\n",
    "            \n",
    "\n",
    "            What the function does?\n",
    "            -----------------------\n",
    "            Determines probability of each test example against all classes and predicts the label\n",
    "            against which the class probability is maximum\n",
    "\n",
    "            Returns:\n",
    "            ---------\n",
    "            Predictions of test examples - A single prediction against every test example\n",
    "        '''       \n",
    "       \n",
    "        predictions=[] #to store prediction of each test example\n",
    "        for example in test_set: \n",
    "                                              \n",
    "            #preprocess the test example the same way we did for training set exampels                                  \n",
    "            cleaned_example=preprocess_string(example) \n",
    "             \n",
    "            #simply get the posterior probability of every example                                  \n",
    "            post_prob=self.getExampleProb(cleaned_example) #get prob of this example for both classes\n",
    "            \n",
    "            #simply pick the max value and map against self.classes!\n",
    "            predictions.append(self.classes[np.argmax(post_prob)])\n",
    "                \n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8B88jxVSvBwJ",
    "outputId": "19f7d0f5-535e-4660-f81b-893766e5201c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Classes:  ['0' '1']\n",
      "Total Number of Training Examples:  (79999,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    '''\n",
    "        Splitting dataset as we don't have seperate test dataset:\n",
    "        -----------\n",
    "        We used sklearn library to split the dataset randomly in to parts\n",
    "        train : 80%\n",
    "        test : 20%\n",
    "\n",
    "        The reason of using a libary was to make sure we have a proper shuffled dataset after splitting. We could have used defined function to do this but this helped in getting better dataset.\n",
    "    '''\n",
    "\n",
    "#getting training set examples labels\n",
    "y_train=training_set['sentiment'].values\n",
    "x_train=training_set['review'].values\n",
    "print (\"Unique Classes: \",np.unique(y_train))\n",
    "print (\"Total Number of Training Examples: \",x_train.shape)    \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data,test_data,train_labels,test_labels=train_test_split(x_train,y_train,shuffle=True,test_size=0.2,random_state=42,stratify=y_train)\n",
    "classes=np.unique(train_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u8yW847HBZoH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Training In Progress------------------------\n",
      "Training Examples:  (63999,)\n",
      "------------------------Training Completed!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "        Training: \n",
    "        -----------\n",
    "        We will be training our model here.\n",
    "        '''\n",
    "nb=NaiveBayes(classes)\n",
    "print (\"------------------Training In Progress------------------------\")\n",
    "print (\"Training Examples: \",train_data.shape)\n",
    "nb.train(train_data,train_labels)\n",
    "print ('------------------------Training Completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lEsvpyhsz3vw"
   },
   "source": [
    "# **Different Algorithms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oySgUM810CGg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('review', 'sentiment')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assigning column names into variables\n",
    "tweet = training_set.columns.values[1]\n",
    "sentiment = training_set.columns.values[0]\n",
    "tweet, sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dKE9iNqi1fWP"
   },
   "source": [
    "**Preprocessing**\n",
    "\n",
    "Convert every tweets to lower case\n",
    "\n",
    "*   Remove @USER username\n",
    "*   Remove punctuations, numbers and special characters\n",
    "*   Convert more than 2 letter repetitions to 2 letter ( example (coooool --> cool))\n",
    "*   Remove extra spaces\n",
    "* Remove @URL\n",
    "* Emoji analysis\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YT50dfXN1TBc"
   },
   "outputs": [],
   "source": [
    "# A function which handles emoji classification\n",
    "\n",
    "def emoji(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :') , :O\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\)|:O)', ' positiveemoji ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' positiveemoji ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' positiveemoji ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-; , @-)\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;|@-\\))', ' positiveemoji ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:, :-/ , :-|\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:|:-/|:-\\|)', ' negetiveemoji ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' negetiveemoji ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YHUDMZzh2MJG"
   },
   "outputs": [],
   "source": [
    "# A function which will preprocess the tweets\n",
    "def process_tweet(tweet):\n",
    "    tweet = tweet.lower()                                             # Lowercases the string\n",
    "    tweet = re.sub('@[^\\s]+', '', tweet)                              # Removes usernames\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', ' ', tweet)   # Remove URLs\n",
    "    tweet = re.sub(r\"\\d+\", \" \", str(tweet))                           # Removes all digits\n",
    "    tweet = re.sub('&quot;',\" \", tweet)                               # Remove (&quot;) \n",
    "    tweet = emoji(tweet)                                              # Replaces Emojis\n",
    "    tweet = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", str(tweet))                   # Removes all single characters\n",
    "    tweet = re.sub(r\"[^\\w\\s]\", \" \", str(tweet))                       # Removes all punctuations\n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)                         # Convert more than 2 letter repetitions to 2 letter\n",
    "    tweet = re.sub(r\"\\s+\", \" \", str(tweet))                           # Replaces double spaces with single space    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uGjrXJUn2Y3q"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>processed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@USER @USER a sicrhau bod mwy o arian poced 'd...</td>\n",
       "      <td>sicrhau bod mwy arian poced da ti nes fe ti e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Parti Dolig da gyda tim swyddfa canolog @USER ...</td>\n",
       "      <td>parti dolig da gyda tim swyddfa canolog ty gwy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@USER yeaah ma fe yn wir. ( oh well.</td>\n",
       "      <td>yeaah ma fe yn wir oh well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>@USER hahaha idk. 3am oedd y bws ti?</td>\n",
       "      <td>hahaha idk am oedd bws ti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@USER dwim yn gal llun ohoni?</td>\n",
       "      <td>dwim yn gal llun ohoni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>@USER desparate housewives! Dadla be di gora h...</td>\n",
       "      <td>desparate housewives dadla be di gora heno gw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>@USER Diolch! Dweud wrtho Manon fi wedi txto h...</td>\n",
       "      <td>diolch dweud wrtho manon fi wedi txto hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>@USER mar boy yn hymian o nappies. Dwim isho s...</td>\n",
       "      <td>mar boy yn hymian nappies dwim isho siarad ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>@USER mor browd, fe fyddai yn dangos ti faint ...</td>\n",
       "      <td>mor browd fe fyddai yn dangos ti faint mor aw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>being a grown-up sucks, dw i 'sho mynd i Vfest .</td>\n",
       "      <td>being grown up sucks dw sho mynd vfest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                             review  \\\n",
       "0         0  @USER @USER a sicrhau bod mwy o arian poced 'd...   \n",
       "1         1  Parti Dolig da gyda tim swyddfa canolog @USER ...   \n",
       "2         0               @USER yeaah ma fe yn wir. ( oh well.   \n",
       "3         1               @USER hahaha idk. 3am oedd y bws ti?   \n",
       "4         0                      @USER dwim yn gal llun ohoni?   \n",
       "5         0  @USER desparate housewives! Dadla be di gora h...   \n",
       "6         1  @USER Diolch! Dweud wrtho Manon fi wedi txto h...   \n",
       "7         0  @USER mar boy yn hymian o nappies. Dwim isho s...   \n",
       "8         1  @USER mor browd, fe fyddai yn dangos ti faint ...   \n",
       "9         0   being a grown-up sucks, dw i 'sho mynd i Vfest .   \n",
       "\n",
       "                                     processed_tweet  \n",
       "0   sicrhau bod mwy arian poced da ti nes fe ti e...  \n",
       "1  parti dolig da gyda tim swyddfa canolog ty gwy...  \n",
       "2                        yeaah ma fe yn wir oh well   \n",
       "3                         hahaha idk am oedd bws ti   \n",
       "4                            dwim yn gal llun ohoni   \n",
       "5   desparate housewives dadla be di gora heno gw...  \n",
       "6          diolch dweud wrtho manon fi wedi txto hi   \n",
       "7   mar boy yn hymian nappies dwim isho siarad ef...  \n",
       "8   mor browd fe fyddai yn dangos ti faint mor aw...  \n",
       "9            being grown up sucks dw sho mynd vfest   "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A new column for side by side comparison of new tweets vs old tweets\n",
    "training_set['processed_tweet'] = np.vectorize(process_tweet)(training_set[tweet])\n",
    "training_set.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8phRH6p42lKp"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "    Analysing sentements on the basis of the data given\n",
    "    0 : Negative\n",
    "    1 : Positive\n",
    "'''\n",
    "\n",
    "sentiments = ['Positive Sentiment', 'Negetive Sentiment'] \n",
    "slices = [(training_set[sentiment] == '1').sum(), (training_set[sentiment] == '0').sum()] \n",
    "colors = ['g', 'r'] \n",
    "plt.pie(slices, labels = sentiments, colors=colors, startangle=90, shadow = True,\n",
    "        explode = (0, 0.1), radius = 1.5, autopct = '%1.2f%%') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RTrSNEQ53FL3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<79999x532794 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1981728 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "      We have to extract or convert the text data into numbers without losing much of the information. \n",
    "      One way to do such transformation is Bag-Of-Words (BOW) which gives a number to each word but that is very inefficient. \n",
    "      So, a way to do it is by CountVectorizer: it counts the number of words in the document \n",
    "      i.e it converts a collection of text documents to a matrix of the counts of occurences of each word in the document.\n",
    "'''\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,2))    # Unigram and Bigram\n",
    "final_vectorized_data = count_vectorizer.fit_transform(training_set['review'])  \n",
    "final_vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TYFeLV3l3hec"
   },
   "outputs": [],
   "source": [
    "    '''\n",
    "        Splitting dataset as we don't have seperate test dataset:\n",
    "        -----------\n",
    "        We used sklearn library to split the dataset randomly in to parts\n",
    "        train : 80%\n",
    "        test : 20%\n",
    "    '''\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_vectorized_data, training_set[sentiment], test_size=0.2, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BGPImjs64WNB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_shape :  (63999, 532794)\n",
      "X_test_shape :  (16000, 532794)\n",
      "y_train_shape :  (63999,)\n",
      "y_test_shape :  (16000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_shape : \",X_train.shape)\n",
    "print(\"X_test_shape : \",X_test.shape)\n",
    "print(\"y_train_shape : \",y_train.shape)\n",
    "print(\"y_test_shape : \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9yG3xkrE4dZg"
   },
   "source": [
    "There are some popular classifiers under Naive Bayes\n",
    "\n",
    "*   Bernoulli Naive Bayes\n",
    "\n",
    "*  Gaussian Naive Bayes classifier\n",
    "*  Multinomial Naive Bayes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We will use **Multinomial Naive Bayes classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GhOhwoLP4XQi"
   },
   "outputs": [],
   "source": [
    "\n",
    "model_naive = MultinomialNB().fit(X_train, y_train) \n",
    "predicted_naive = model_naive.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qoYdI0RU6Sjr"
   },
   "source": [
    "Here we **LogisticRegression** to compare accuracy. \n",
    "\n",
    "Regularization (C) = 0.5 gave best accuracy. \n",
    "\n",
    "We tested for C = [0.01, 0.05, 0.25, 0.5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rejofFo656EM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5, max_iter=7600)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=0.5,max_iter=7600,solver='lbfgs')\n",
    "lr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RTrxqvrx7Ds3"
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "  #Calculating accuracy for Scratch part :\n",
    "  print (\"------------------Navive Bayes from Scratch------------------------\")\n",
    "  pclasses=nb.test(test_data)\n",
    "  test_acc=np.sum(pclasses==test_labels)/float(test_labels.shape[0])\n",
    "  print (\"Accuracy with Navive Bayes from Scratch: \",test_acc)\n",
    "\n",
    "  print('\\n')\n",
    "\n",
    "  #Calculating accuracy for Multinomial Naive Bayes :\n",
    "  print (\"------------------Multinomial Naive Bayes------------------------\")\n",
    "  score_naive = accuracy_score(predicted_naive, y_test)\n",
    "  print(\"Accuracy with Multinomial Naive Bayes: \",score_naive)\n",
    "\n",
    "  print('\\n')\n",
    "\n",
    "  #Calculating accuracy for Logistic Regression :\n",
    "  print (\"------------------Logistic Regression------------------------\")\n",
    "  score_logistic = accuracy_score(y_test, lr.predict(X_test))\n",
    "  print(\"Accuracy with Logistic Regression: \",score_logistic)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hc6hdvyfEcZV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Navive Bayes from Scratch------------------------\n",
      "Accuracy with Navive Bayes from Scratch:  0.736125\n",
      "\n",
      "\n",
      "------------------Multinomial Naive Bayes------------------------\n",
      "Accuracy with Multinomial Naive Bayes:  0.7465\n",
      "\n",
      "\n",
      "------------------Logistic Regression------------------------\n",
      "Accuracy with Logistic Regression:  0.7591875\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qbBc65hPvBwW"
   },
   "outputs": [],
   "source": [
    "# def evaluate():\n",
    "#   total = 0\n",
    "#   correct_from_scratch = 0\n",
    "#   correct_anything_goes = 0\n",
    "#   testfile = open('test.tsv', 'r')\n",
    "#   for line in testfile:\n",
    "#     total += 1\n",
    "#     pieces = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "#     if nb.test(pieces[1]) == int(pieces[0]):\n",
    "#       correct_from_scratch += 1\n",
    "#     if predict_anything_goes(pieces[1]) == int(pieces[0]):\n",
    "#       correct_anything_goes += 1\n",
    "#   return (correct_from_scratch/total, correct_anything_goes/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oUEH5voKN3md"
   },
   "source": [
    "# **Acknowledgement :**\n",
    "\n",
    "\n",
    "\n",
    "*  [Dr. Kevin Scannell](https://github.com/kscanne)\n",
    "*   [Medium](https://medium.com/@martinpella/naive-bayes-for-sentiment-analysis-49b37db18bf8)\n",
    "* [StackShare](https://stackabuse.com/the-naive-bayes-algorithm-in-python-with-scikit-learn/)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WelshSentiment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
