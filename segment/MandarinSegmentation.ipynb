{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2XhBpSvzjel"
   },
   "source": [
    "# **Mandarin Word Segmentation**\n",
    "\n",
    "![](https://i.imgur.com/Y9JDMFK.jpg)\n",
    "\n",
    "\n",
    "\n",
    "> **The code here are divided in 2 parts**\n",
    "\n",
    "*   Done using Viterbi Algorithm\n",
    "*   Used CRFsuite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKh4IBfRzg1h"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import nltk, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from IPython.display import HTML, display\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBWaQbDBzg1n"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "        Data Loading TRAIN:\n",
    "        ----------\n",
    "        The data is loaded from a tsv file\n",
    "        ---------\n",
    "\n",
    "        Creating Sentences: \n",
    "        ----------\n",
    "        For tags the sentences are created using \" 。\" to define end of a sentence\n",
    "        ----------\n",
    "'''\n",
    "whole_text = []\n",
    "\n",
    "def tagSetupTrain(): \n",
    "  testfile = open('train.tsv', 'r')\n",
    "  sentence = []\n",
    "  for line in testfile:\n",
    "    pieces = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "    if pieces[0]=='。':\n",
    "      whole_text.append((sentence))\n",
    "      sentence = []\n",
    "    else:\n",
    "      sentence.append(tuple(pieces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-A5zpPdzg1u"
   },
   "outputs": [],
   "source": [
    "tagSetupTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0sg1p-czg10",
    "outputId": "5d592673-88f7-47db-e872-5cbb828b9bc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179491"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(whole_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9OtR8Jrzg16"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "        Data Loading TEST:\n",
    "        ----------\n",
    "        The data is loaded from a tsv file\n",
    "        ---------\n",
    "\n",
    "        Creating Sentences: \n",
    "        ----------\n",
    "        For tags the sentences are created using \" 。\" to define end of a sentence\n",
    "        ----------\n",
    "'''\n",
    "whole_test_text = []\n",
    "def tagSetupTest():\n",
    "    testfile = open('test.tsv', 'r')\n",
    "    sentence_test = []\n",
    "    for line in testfile:\n",
    "        pieces = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        if pieces[0]=='。':\n",
    "          whole_test_text.append((sentence_test))\n",
    "          sentence_test = []\n",
    "        else:\n",
    "          sentence_test.append(tuple(pieces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3VG4hQCazg2B"
   },
   "outputs": [],
   "source": [
    "tagSetupTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9nSWEB6zg2H"
   },
   "outputs": [],
   "source": [
    "train_set,test_set = whole_text, whole_test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LGDdRhyzg2O",
    "outputId": "4b2e45ca-cdc1-4144-b45e-7ee3eed03801"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Training Set Length - 179491\n",
      "Testing Set Length - 3351\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training Data Glimpse -\n",
      "\n",
      "[[('時', '0'), ('間', '1'), ('：', '1'), ('三', '0'), ('月', '1'), ('十', '0'), ('日', '1'), ('（', '1'), ('星', '0'), ('期', '0'), ('四', '1'), ('）', '1'), ('上', '0'), ('午', '1'), ('十', '0'), ('時', '1')]]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 100)\n",
    "print(\"Training Set Length -\", len(train_set))\n",
    "print(\"Testing Set Length -\", len(test_set))\n",
    "print(\"-\" * 100)\n",
    "print(\"Training Data Glimpse -\\n\")\n",
    "print(train_set[:1])\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pd4rzCHmzg2U",
    "outputId": "4ebcb025-1609-43ee-e496-b0bf80a7e852"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8188676\n",
      "194345\n"
     ]
    }
   ],
   "source": [
    "# create list of train and test tagged words\n",
    "train_tagged_words = [tup for sent in train_set for tup in sent]\n",
    "test_tagged_words = [tup[0] for sent in test_set for tup in sent]\n",
    "print(len(train_tagged_words))\n",
    "print(len(test_tagged_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlIS01_czg2b",
    "outputId": "9524de6e-a1b7-430f-c821-0f783d6e5ea1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('間', '1'), ('：', '1'), ('三', '0'), ('月', '1')]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check some of the tagged words.\n",
    "train_tagged_words[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8iCjdcpzg2g",
    "outputId": "f6a21a7a-373e-4a85-a835-cebb604d6f11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{'0', '1'}\n"
     ]
    }
   ],
   "source": [
    "# let's check how many unique tags are present in training data\n",
    "tags = {tag for word,tag in train_tagged_words}\n",
    "print(len(tags))\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LouX7yi6zg2k",
    "outputId": "b22912fd-547d-4bf8-ca55-0b9ebb6632bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6115\n"
     ]
    }
   ],
   "source": [
    "# let's check how many words are present in vocabulary\n",
    "vocab = {word for word,tag in train_tagged_words}\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2o0XsJ3uzg2p"
   },
   "outputs": [],
   "source": [
    "# compute emission probability for a given word for a given tag\n",
    "def word_given_tag(word,tag,train_bag= train_tagged_words):\n",
    "    \"\"\"\"\n",
    "        Parameters:\n",
    "        ----------\n",
    "        word: individualw word w\n",
    "        train_bag: it is the training set that we initialized at top.\n",
    "        \n",
    "        What the function does?\n",
    "        -----------------------\n",
    "        It computes emission probabilties for a given word.\n",
    "        \n",
    "    \"\"\"\n",
    "    taglist = [pair for pair in train_bag if pair[1] == tag]\n",
    "    tag_count = len(taglist)    \n",
    "    w_in_tag = [pair[0] for pair in taglist if pair[0]==word]    \n",
    "    word_count_given_tag = len(w_in_tag)    \n",
    "    \n",
    "    return (word_count_given_tag,tag_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhEPzFsozg2u"
   },
   "outputs": [],
   "source": [
    "# compute transition probabilities of a previous and next tag\n",
    "def t2_given_t1(t2,t1,train_bag=train_tagged_words):\n",
    "    \"\"\"\"\n",
    "        Parameters:\n",
    "        ----------\n",
    "        t2: tag\n",
    "        t1: tag\n",
    "        train_bag: it is the training set that we initialized at top.\n",
    "        \n",
    "        What the function does?\n",
    "        -----------------------\n",
    "        It ompute transition probabilities of a previous and next tag\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    t1_tags = [tag for tag in tags if tag==t1]\n",
    "    count_of_t1 = len(t1_tags)\n",
    "    t2_given_t1 = [tags[index+1] for index in range(len(tags)-1) if tags[index] == t1 and tags[index+1] == t2]\n",
    "    count_t2_given_t1 = len(t2_given_t1)\n",
    "    return(count_t2_given_t1,count_of_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-XL17l2zg2y"
   },
   "outputs": [],
   "source": [
    "# creating t x t transition matrix of tags\n",
    "# each column is t2, each row is t1\n",
    "# thus M(i, j) represents P(tj given ti)\n",
    "\n",
    "tags_matrix = np.zeros((len(tags), len(tags)), dtype='float32')\n",
    "for i, t1 in enumerate(list(tags)):\n",
    "    for j, t2 in enumerate(list(tags)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cjc5aiKzg22",
    "outputId": "eb88bfad-e858-4035-b5c4-c3bb2dc8e491"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.159006</td>\n",
       "      <td>0.840994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.465715</td>\n",
       "      <td>0.534285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.159006  0.840994\n",
       "1  0.465715  0.534285"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the matrix to a df for better readability\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(tags), index=list(tags))\n",
    "\n",
    "# dataset glimpse\n",
    "tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKtRmWzezg26"
   },
   "outputs": [],
   "source": [
    "# # Let's test our Viterbi algorithm on the sample sentences of test dataset. We are using sample senetences to minimize server crash\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "# choose random 5 sents\n",
    "rndom = [random.randint(1,len(test_set)) for x in range(20)]\n",
    "\n",
    "# list of sents\n",
    "test_run = [test_set[i] for i in rndom]\n",
    "\n",
    "# list of tagged words\n",
    "test_run_base = [tup for sent in test_run for tup in sent]\n",
    "\n",
    "# list of untagged words\n",
    "test_tagged_words = [tup[0] for sent in test_run for tup in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kq0ghhSizg2_"
   },
   "outputs": [],
   "source": [
    "def Viterbi_1(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        p_transition =[] # list for storing transition probabilities\n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = 0\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            p_transition.append(transition_p)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        state_max = T[p.index(pmax)] \n",
    "        \n",
    "      \n",
    "        # if probability is zero (unknown word) then use transition probability\n",
    "        if(pmax==0):\n",
    "            pmax = max(p_transition)\n",
    "            state_max = T[p_transition.index(pmax)]\n",
    "                           \n",
    "        else:\n",
    "            state_max = T[p.index(pmax)] \n",
    "        \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuaXHWp1zg3D"
   },
   "outputs": [],
   "source": [
    "tagged_seq_v1 = Viterbi_1(test_tagged_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKp78BPGzg3H",
    "outputId": "84a2d1b3-3881-4459-b9c1-9a0bc88ee957"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified Viterbi_1 Accuracy:  76.93920335429769\n"
     ]
    }
   ],
   "source": [
    "check_v1 = [i for i, j in zip(tagged_seq_v1, test_run_base) if i == j] \n",
    "accuracy_v1 = len(check_v1)/len(tagged_seq_v1)\n",
    "print('Modified Viterbi_1 Accuracy: ',accuracy_v1*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2Q2nz9pzg3M",
    "outputId": "c402aade-31c9-4fa8-de7b-88e3febc2630"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.65      0.70       396\n",
      "           1       0.78      0.85      0.81       558\n",
      "\n",
      "    accuracy                           0.77       954\n",
      "   macro avg       0.77      0.75      0.76       954\n",
      "weighted avg       0.77      0.77      0.77       954\n",
      "\n",
      "[[259 137]\n",
      " [ 83 475]]\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "true = []\n",
    "for i, j in (tagged_seq_v1):\n",
    "#     print(j)\n",
    "    pred.append(j)\n",
    "for i, j in (test_run_base):\n",
    "#     print(j)\n",
    "    true.append(j)\n",
    "target_names = ['0', '1']\n",
    "print(classification_report(true,pred, target_names=target_names))\n",
    "print(confusion_matrix(true, pred, labels=[\"0\", \"1\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8_-xbwx0y1E"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yj0a10gYzg3R"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import pycrfsuite\n",
    "import urllib.request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LIO52wSzg3U"
   },
   "outputs": [],
   "source": [
    "# Importing data \n",
    "\n",
    "tagSetupTrain()\n",
    "tagSetupTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wwI3BWRzg3Y"
   },
   "outputs": [],
   "source": [
    "# Assigning data to variables to use \n",
    "\n",
    "prepared_sentences,prepared_test_sentences = whole_text,whole_test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AooZnfmLzg3d",
    "outputId": "340fc26f-e734-4ff8-bedf-4b8c7ebd3957"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('楊', '0'), ('建', '1'), ('為', '0'), ('了', '1'), ('沒', '0'), ('有', '1'), ('好', '0'), ('好', '1'), ('保', '0'), ('存', '1'), ('父', '0'), ('親', '1'), ('的', '1'), ('手', '0'), ('稿', '1'), ('，', '1'), ('向', '1'), ('台', '0'), ('灣', '1'), ('社', '0'), ('會', '1'), ('大', '0'), ('眾', '1'), ('鞠', '0'), ('躬', '1'), ('道', '0'), ('歉', '1')]\n"
     ]
    }
   ],
   "source": [
    "# Running Sample data\n",
    "\n",
    "print([d for d in prepared_test_sentences[21]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwV8CI9S1P-P"
   },
   "source": [
    "**Transforming the characters to feature vectors.**\n",
    "\n",
    "Finally, we can create some simple n-gram features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVzW2saMzg3i"
   },
   "outputs": [],
   "source": [
    "#here sentence is prepared_sentence and i is length of prepared_sentence\n",
    "def create_char_features(sentence, i):\n",
    "    #set initial feature set char as first char in prepared_sentence\n",
    "    features = [\n",
    "        'bias',\n",
    "        'char=' + sentence[i][0] \n",
    "    ]\n",
    "    #if i >=1 then go to previous character else append 'BOS' in features list \n",
    "    if i >= 1:\n",
    "        features.extend([\n",
    "            'char-1=' + sentence[i-1][0],\n",
    "            'char-1:0=' + sentence[i-1][0] + sentence[i][0],\n",
    "        ])\n",
    "    else:\n",
    "        features.append(\"BOS\")\n",
    "        \n",
    "    if i >= 2:\n",
    "        features.extend([\n",
    "            'char-2=' + sentence[i-2][0],\n",
    "            'char-2:0=' + sentence[i-2][0] + sentence[i-1][0] + sentence[i][0],\n",
    "            'char-2:-1=' + sentence[i-2][0] + sentence[i-1][0],\n",
    "        ])\n",
    "        \n",
    "    if i >= 3:\n",
    "        features.extend([\n",
    "            'char-3:0=' + sentence[i-3][0] + sentence[i-2][0] + sentence[i-1][0] + sentence[i][0],\n",
    "            'char-3:-1=' + sentence[i-3][0] + sentence[i-2][0] + sentence[i-1][0],\n",
    "        ])\n",
    "    #if i+1 < len(sentence) then go to next character and set it to next character and set char to next two characters else append 'EOS' to features list\n",
    "    if i + 1 < len(sentence):\n",
    "        features.extend([\n",
    "            'char+1=' + sentence[i+1][0],\n",
    "            'char:+1=' + sentence[i][0] + sentence[i+1][0],\n",
    "        ])\n",
    "    else:\n",
    "        features.append(\"EOS\")\n",
    "    #if first if condition satisfy then go to second and third if condition and do the same work for next characters    \n",
    "    if i + 2 < len(sentence):\n",
    "        features.extend([\n",
    "            'char+2=' + sentence[i+2][0],\n",
    "            'char:+2=' + sentence[i][0] + sentence[i+1][0] + sentence[i+2][0],\n",
    "            'char+1:+2=' + sentence[i+1][0] + sentence[i+2][0],\n",
    "        ])\n",
    "    \n",
    "    if i + 3 < len(sentence):\n",
    "        features.extend([\n",
    "            'char:+3=' + sentence[i][0] + sentence[i+1][0] + sentence[i+2][0]+ sentence[i+3][0],\n",
    "            'char+1:+3=' + sentence[i+1][0] + sentence[i+2][0] + sentence[i+3][0],\n",
    "        ])\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def create_sentence_features(prepared_sentence):\n",
    "    return [create_char_features(prepared_sentence, i) for i in range(len(prepared_sentence))]\n",
    "\n",
    "def create_sentence_labels(prepared_sentence):\n",
    "    return [str(part[1]) for part in prepared_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvSE-QDfzg3l"
   },
   "outputs": [],
   "source": [
    "# Assigning data for running test cases\n",
    "X = [create_sentence_features(ps) for ps in prepared_sentences]\n",
    "y = [create_sentence_labels(ps)   for ps in prepared_sentences]\n",
    "\n",
    "X_test = [create_sentence_features(ps) for ps in prepared_test_sentences]\n",
    "y_test = [create_sentence_labels(ps)   for ps in prepared_test_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdAaf02k1nel"
   },
   "source": [
    "**Training a CRF**\n",
    "\n",
    "Now, we use Python-CRFSuite for training a CRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKwTpw4szg3q"
   },
   "outputs": [],
   "source": [
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X, y):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2OBQEpZzg3t"
   },
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 1.0, \n",
    "    'c2': 1e-3,\n",
    "    'max_iterations': 60,\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWsKjBcMzg3x",
    "outputId": "0a489fa2-a342-4585-adea-847e215ca03d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pycrfsuite._pycrfsuite.Trainer"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxThKLmMzg30"
   },
   "outputs": [],
   "source": [
    "#training model \n",
    "\n",
    "trainer.train('mandarin-text-segmentation.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCyM2Ym4zg34",
    "outputId": "ff9edc2e-264c-4042-9f27-0ee7d3fb0de9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7f47cdc824c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#open trained model\n",
    "\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('mandarin-text-segmentation.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNUCna0Azg38"
   },
   "outputs": [],
   "source": [
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "n_correct = 0\n",
    "n_incorrect = 0\n",
    "\n",
    "for s in prepared_test_sentences:\n",
    "    prediction = tagger.tag(create_sentence_features(s))\n",
    "    correct = create_sentence_labels(s)\n",
    "    zipped = list(zip(prediction, correct))\n",
    "    tp +=        len([_ for l, c in zipped if l == c and l == \"1\"])\n",
    "    fp +=        len([_ for l, c in zipped if l == \"1\" and c == \"0\"])\n",
    "    fn +=        len([_ for l, c in zipped if l == \"0\" and c == \"1\"])\n",
    "    n_incorrect += len([_ for l, c in zipped if l != c])\n",
    "    n_correct   += len([_ for l, c in zipped if l == c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oniTl_7Ezg4A",
    "outputId": "54a02953-6cf4-4df9-efb0-784514e35ed0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:\t0.9689790662352242\n",
      "Recall:\t\t0.9841709006153898\n",
      "Accuracy:\t0.9709485708405156\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision:\\t\" + str(tp/(tp+fp)))\n",
    "print(\"Recall:\\t\\t\" + str(tp/(tp+fn)))\n",
    "print(\"Accuracy:\\t\" + str(n_correct/(n_correct+n_incorrect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49GhWlx_zg4E"
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    print('Part 1 - Virtebi')\n",
    "    print(classification_report(true,pred, target_names=target_names))\n",
    "    print()\n",
    "    print('Part 2 ')\n",
    "    print(\"Precision:\\t\" + str(tp/(tp+fp)))\n",
    "    print(\"Recall:\\t\\t\" + str(tp/(tp+fn)))\n",
    "    print(\"Accuracy:\\t\" + str(n_correct/(n_correct+n_incorrect)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "MandarinSegmentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
